{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'special' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ddb59971e01c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[0menglish\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mkorean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[0mspecial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'special' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "nlpy = Kkma()\n",
    "file = open('hashcode_classification2020_train.csv', 'r', encoding='utf-8')\n",
    "line = csv.reader(file)\n",
    "\n",
    "f = open(\"goodWordsk.txt\", 'w')\n",
    "\n",
    "english=open(\"English_Dict.txt\", 'r')\n",
    "korean=open(\"Korean_Dict.txt\", 'r')\n",
    "\n",
    "englishDict=[]\n",
    "sentences = english.readlines()\n",
    "for sentence in sentences: #한 문장씩\n",
    "    englishDict.append(sentence[:-1])\n",
    "\n",
    "koreanDict=[]\n",
    "sentences = korean.readlines()\n",
    "for sentence in sentences: #한 문장씩\n",
    "    koreanDict.append(sentence[:-1])\n",
    "\n",
    "\n",
    "for line_text in line:\n",
    "    eng=re.sub('[^a-zA-Z]',' ',line_text[0]+line_text[1]).strip()\n",
    "    eng_temp=eng.split(' ')\n",
    "    eng_temp = list(filter(None, eng_temp))\n",
    "    for i in range(len(eng_temp)):\n",
    "        if eng_temp[i] not in englishDict:\n",
    "            eng_temp[i]=''\n",
    "    eng_temp = list(filter(None, eng_temp))\n",
    "    \n",
    "    kor=nlpy.nouns(line_text[0]+line_text[1])\n",
    "    for i in range(len(kor)):\n",
    "        if kor[i] not in koreanDict:\n",
    "            kor[i]=''\n",
    "    kor = list(filter(None, kor))\n",
    "    \n",
    "    \n",
    "    s=' '.join(eng_temp)+' '+' '.join(kor)+','+line_text[2]\n",
    "    f.write(s)\n",
    "    f.write('\\n')\n",
    "    \n",
    "    \n",
    "#     kor=\n",
    "file.close()\n",
    "f.close()\n",
    "english.close()\n",
    "korean.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': 'wait sleep wait sleep ', 'flag': '3'}\n",
      "(2592,)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import csv\n",
    "file= open('goodWordsk.txt', 'r')\n",
    "line = file.readline()\n",
    "training_data=[]\n",
    "while line:\n",
    "    data=[line[:-3],line[-2:-1]]\n",
    "    training_data.append({'data': data[0], 'flag': data[1]})\n",
    "    line=file.readline()\n",
    "print(training_data[0])\n",
    "\n",
    "training_data=pandas.DataFrame(training_data, columns=['data', 'flag'])\n",
    "training_data.to_csv(\"train_data.csv\", sep=',', encoding='utf-8')\n",
    "print(training_data.data.shape)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "#GET VECTOR COUNT\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(training_data.data)\n",
    "\n",
    "#SAVE WORD VECTOR\n",
    "pickle.dump(count_vect.vocabulary_, open(\"count_vector.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#TRANSFORM WORD VECTOR TO TF IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "#SAVE TF-IDF\n",
    "pickle.dump(tfidf_transformer, open(\"tfidf.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#clf = MultinomialNB().fit(X_train_tfidf, training_data.flag)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.flag, test_size=0.01, random_state=42)\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "#SAVE MODEL\n",
    "pickle.dump(clf, open(\"nb_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "docs_new = \"numpy as np print array lst import coding arr utf\"\n",
    "docs_new = [docs_new]\n",
    "\n",
    "#LOAD MODEL\n",
    "loaded_vec = CountVectorizer(vocabulary=pickle.load(open(\"count_vector.pkl\", \"rb\")))\n",
    "loaded_tfidf = pickle.load(open(\"tfidf.pkl\",\"rb\"))\n",
    "loaded_model = pickle.load(open(\"nb_model.pkl\",\"rb\"))\n",
    "\n",
    "X_new_counts = loaded_vec.transform(docs_new)\n",
    "X_new_tfidf = loaded_tfidf.transform(X_new_counts)\n",
    "predicted = loaded_model.predict(X_new_tfidf)\n",
    "\n",
    "print(predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3  -  3\n",
      "5  -  5\n",
      "3  -  3\n",
      "4  -  4\n",
      "1  -  1\n",
      "5  -  5\n",
      "3  -  3\n",
      "4  -  4\n",
      "5  -  5\n",
      "5  -  5\n",
      "3  -  3\n",
      "5  -  5\n",
      "3  -  3\n",
      "3  -  3\n",
      "4  -  4\n",
      "3  -  3\n",
      "4  -  4\n",
      "2  -  2\n",
      "1  -  1\n",
      "5  -  5\n",
      "4  -  4\n",
      "5  -  5\n",
      "3  -  3\n",
      "1  -  1\n",
      "5  -  5\n",
      "3  -  3\n"
     ]
    }
   ],
   "source": [
    "predicted = loaded_model.predict(X_test)\n",
    "result_bayes = pandas.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})\n",
    "result_bayes.to_csv('res_bayes.csv', sep = ',')\n",
    "\n",
    "for predicted_item, result in zip(predicted, y_test):\n",
    "    print(predicted_item, ' - ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 9 0 0]\n",
      " [0 0 0 5 0]\n",
      " [0 0 0 0 8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix  \n",
    "\n",
    "confusion_mat = confusion_matrix(y_test,predicted)\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(15,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_neural = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.flag, test_size=0.01, random_state=42)\n",
    "\n",
    "clf_neural.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf_neural, open(\"softmax.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3  -  3\n",
      "5  -  5\n",
      "3  -  3\n",
      "4  -  4\n",
      "2  -  1\n",
      "5  -  5\n",
      "3  -  3\n",
      "4  -  4\n",
      "5  -  5\n",
      "5  -  5\n",
      "3  -  3\n",
      "5  -  5\n",
      "3  -  3\n",
      "3  -  3\n",
      "4  -  4\n",
      "3  -  3\n",
      "4  -  4\n",
      "2  -  2\n",
      "1  -  1\n",
      "5  -  5\n",
      "4  -  4\n",
      "5  -  5\n",
      "3  -  3\n",
      "1  -  1\n",
      "5  -  5\n",
      "3  -  3\n"
     ]
    }
   ],
   "source": [
    "predicted = clf_neural.predict(X_test)\n",
    "result_softmax = pandas.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})\n",
    "result_softmax.to_csv('res_softmax.csv', sep = ',')\n",
    "\n",
    "for predicted_item, result in zip(predicted, y_test):\n",
    "    print(predicted_item, ' - ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf_svm = svm.LinearSVC()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.flag, test_size=0.01, random_state=42)\n",
    "clf_svm.fit(X_train_tfidf, training_data.flag)\n",
    "pickle.dump(clf_svm, open(\"svm.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3  -  3\n",
      "5  -  5\n",
      "3  -  3\n",
      "4  -  4\n",
      "1  -  1\n",
      "5  -  5\n",
      "3  -  3\n",
      "4  -  4\n",
      "5  -  5\n",
      "5  -  5\n",
      "3  -  3\n",
      "5  -  5\n",
      "3  -  3\n",
      "3  -  3\n",
      "4  -  4\n",
      "3  -  3\n",
      "4  -  4\n",
      "2  -  2\n",
      "1  -  1\n",
      "5  -  5\n",
      "4  -  4\n",
      "5  -  5\n",
      "3  -  3\n",
      "1  -  1\n",
      "5  -  5\n",
      "3  -  3\n"
     ]
    }
   ],
   "source": [
    "predicted = clf_svm.predict(X_test)\n",
    "result_svm = pandas.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})\n",
    "result_svm.to_csv('res_svm.csv', sep = ',')\n",
    "for predicted_item, result in zip(predicted, y_test):\n",
    "    print(predicted_item, ' - ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new=[]\n",
    "f=open(\"testwords.txt\",'r')\n",
    "sentences = f.readlines()\n",
    "for sentence in sentences:\n",
    "    docs_new.append([sentence[:-1]])\n",
    "f.close()\n",
    "f2 = open('output2.csv', 'w', newline='')\n",
    "wr = csv.writer(f2)\n",
    "wr.writerow(['label'])\n",
    "for docs in docs_new:\n",
    "    X_new_counts = loaded_vec.transform(docs)\n",
    "    X_new_tfidf = loaded_tfidf.transform(X_new_counts)\n",
    "    predicted = clf_svm.predict(X_new_tfidf)\n",
    "    wr.writerow([predicted[0]])\n",
    "\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new=[]\n",
    "f=open(\"testwords.txt\",'r')\n",
    "sentences = f.readlines()\n",
    "for sentence in sentences:\n",
    "    docs_new.append([sentence[:-1]])\n",
    "f.close()\n",
    "f2 = open('output2.csv', 'w', newline='')\n",
    "wr = csv.writer(f2)\n",
    "wr.writerow(['label'])\n",
    "for docs in docs_new:\n",
    "    X_new_counts = loaded_vec.transform(docs)\n",
    "    X_new_tfidf = loaded_tfidf.transform(X_new_counts)\n",
    "    predicted = clf_neural.predict(X_new_tfidf)\n",
    "    wr.writerow([predicted[0]])\n",
    "\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new=[]\n",
    "f=open(\"testwords.txt\",'r')\n",
    "sentences = f.readlines()\n",
    "for sentence in sentences:\n",
    "    docs_new.append([sentence[:-1]])\n",
    "f.close()\n",
    "f2 = open('output2.csv', 'w', newline='')\n",
    "wr = csv.writer(f2)\n",
    "wr.writerow(['label'])\n",
    "for docs in docs_new:\n",
    "    X_new_counts = loaded_vec.transform(docs)\n",
    "    X_new_tfidf = loaded_tfidf.transform(X_new_counts)\n",
    "    predicted = loaded_model.predict(X_new_tfidf)\n",
    "    wr.writerow([predicted[0]])\n",
    "\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
